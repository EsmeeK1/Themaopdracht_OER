{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OER-Bot Themaopdracht\n",
    "\n",
    "### Notebook door Groep High Five\n",
    "### Note: Sommige code is geschreven met ChatGPT, dit zal vermeld staan in de tekst boven/ rondom de code.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introductie\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "In dit document wordt de toepassing van GUFF met 'geitje' in combinatie met een semDB verkend. Het doel is om een chatbot te creëren die vragen kan beantwoorden over het Onderwijs- en Examenreglement (OER). We leggen stapsgewijs uit hoe we het OER-bot project hebben aangepakt volgens de CRISP-DM-methode.\n",
    "\n",
    "We beginnen met de **Business Understanding**, waarin we de casus en de eindvisie toelichten. Vervolgens bespreken we de **Data Understanding**, waar we ingaan op het aangeleverde Word-bestand en de bevindingen die we hebben gedaan. Bij de **Data Preparation** leggen we uit hoe we het Word-bestand hebben omgezet naar een bruikbaar formaat voor ons model. In de sectie **Modelling** beschrijven we hoe we zijn overgegaan van Mischa's notebook naar een Flask-app. Bij de **Evaluation** reflecteren we op het eindresultaat, en in de sectie **Deployment** bespreken we de beste vervolgstappen voor de Flask-app.\n",
    "\n",
    "Deze opdracht is afkomstig van Hogeschool Windesheim, een Nederlandse instelling voor hoger onderwijs. De hogeschool biedt een breed scala aan bachelor- en masteropleidingen, waaronder programma's in techniek en sociale wetenschappen. Hogeschool Windesheim heeft verschillende vestigingen en telt ongeveer 1800 medewerkers in totaal.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Understanding\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "Windesheim ondervindt dat studenten het Onderwijs- en Examenreglement (OER) onvoldoende bestuderen. Dit gebrek aan begrip kan leiden tot verwarring en onzekerheid bij studenten over hun rechten en plichten binnen het onderwijs. Om deze uitdaging aan te pakken, wil Windesheim een chatbot ontwikkelen die studenten helpt bij het beantwoorden van hun vragen over het OER. Deze chatbot biedt een gebruiksvriendelijke manier voor studenten om informatie te verkrijgen, waardoor zij beter in staat zijn om belangrijke details uit het OER tot zich te nemen.\n",
    "\n",
    "### Oplossing\n",
    "De chatbot biedt een oplossing om de toegankelijkheid van de informatie in het OER te verbeteren. Door middel van een chatbot kunnen studenten eenvoudig vragen stellen en direct antwoorden ontvangen. Dit helpt hen om op een gemakkelijkere en efficiëntere manier inzicht te krijgen in de inhoud van het OER, wat hen in staat stelt om beter geïnformeerde keuzes te maken over hun studie. De chatbot fungeert als een toegankelijke bron van informatie die altijd beschikbaar is.\n",
    "\n",
    "### Kritische Succesfactoren\n",
    "Het project is succesvol wanneer er een volledig functionele chatbot is ontwikkeld binnen een Python-applicatie, inclusief een grafische gebruikersinterface (GUI) met Flask. Deze chatbot moet in staat zijn om relevante vragen te beantwoorden met informatie uit het OER-Bestand.\n",
    "\n",
    "### Context\n",
    "De context van dit project is het ontwikkelen van een chatbot die specifiek gericht is op het helpen van studenten bij het vinden van informatie over het OER. Veel studenten hebben moeite om deze documenten volledig te begrijpen, wat kan leiden tot onduidelijkheden en frustraties. Door een chatbot te implementeren, willen we deze obstakels verlagen en studenten in staat stellen om de informatie die zij nodig hebben gemakkelijk te vinden en te begrijpen.\n",
    "\n",
    "### Ontwerpvisie\n",
    "De eindvisie van het project is om een werkende chatbot te creëren die studenten in staat stelt om vragen te stellen over het OER. Het doel is om de drempel voor het begrijpen van deze belangrijke documenten te verlagen, zodat studenten zich beter kunnen oriënteren en informeren over hun rechten en plichten binnen het onderwijs.\n",
    "\n",
    "### 'Dataset'\n",
    "De 'dataset' die wordt gebruikt in dit project is het Onderwijs- en Examenreglement in Word-formaat. Het is essentieel om te onderzoeken hoe we deze gegevens effectief kunnen omzetten naar een formaat dat bruikbaar is voor de chatbot.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Understanding\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "Voor deze themaopdracht pakken we het proces van de data understanding anders aan. We hebben een Word-bestand gekregen van het Onderwijs- en Examenreglement (OER), dat we op een zinvolle manier willen omzetten naar een semantische database (SemDB). Als eerste hebben we dit document gezamenlijk doorgenomen en gekeken naar de structuur en de inhoud.\n",
    "\n",
    "Al snel kwamen we tot de conclusie dat bepaalde onderdelen in het document niet relevant of bruikbaar zijn voor SemDB.\n",
    "\n",
    "Niet-relevante onderdelen:\n",
    "- Afbeeldingen\n",
    "- Inhoud\n",
    "- Tabellen\n",
    "- Voorblad\n",
    "\n",
    "``Afbeeldingen``: Afbeeldingen zijn voor ons model niet van toegevoegde waarde, aangezien de bot enkel tekstuele vragen over het OER gaat beantwoorden.\n",
    "\n",
    "![afbeelding](Afbeeldingen\\afbeelding.JPG)\n",
    "\n",
    "``Inhoud``: De inhoudsopgave bevat alleen kopjes en hoofdstuktitels, wat geen relevante informatie oplevert voor het model. Het is daarom belangrijk om deze in de data preparation te verwijderen.\n",
    "\n",
    "![inhoud](Afbeeldingen\\inhoud.JPG)\n",
    "\n",
    "``Tabellen``: De tabellen bevatten waardevolle informatie voor de chatbot. Echter staan de gegevens momenteel in tabelvorm, en deze moeten worden omgezet naar tekst. We moeten onderzoeken hoe we dit efficiënt kunnen omzetten.\n",
    "\n",
    "![Tabellen](Afbeeldingen\\Tabel.JPG)\n",
    "\n",
    "``Voorblad``: Het voorblad levert geen bruikbare informatie op en kan in de data preparation worden verwijderd.\n",
    "\n",
    "Verder moeten we onderzoeken of we het Word-bestand kunnen gebruiken zoals het is, of dat we het moeten converteren naar een tekstbestand (.txt).\n",
    "\n",
    "Actie punten voor data prep:\n",
    "- Afbeeldingen verwijderen.\n",
    "- Inhoud verwijderen\n",
    "- Voorblad verwijderen\n",
    "\n",
    "Daarnaast moeten we een snelle en effectieve manier vinden om de tabellen om te zetten naar tekst, aangezien deze informatie te belangrijk is om weg te laten. We moeten dus onderzoeken of we dit handmatig of automatisch kunnen uitvoeren.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "In eerste instantie kozen we ervoor om het Word-bestand om te zetten naar een .txt-bestand. Dit deden we omdat we de code van Mischa gebruikte die .txt-bestanden gebruikte, waardoor we snel konden testen of de chatbot werkte. Later bleek echter dat dit niet de ideale manier was. Hieronder staat de eerste data preparation, met de latere oplossing waarbij we geen .txt meer hebben gebruikt.\n",
    "\n",
    "Tijdens de data understanding ontdekten we dat bepaalde onderdelen uit het document verwijderd moesten worden. Aangezien het weinig tijd kostte om afbeeldingen, het voorblad en de inhoudsopgave te verwijderen, besloten we dit handmatig te doen.\n",
    "\n",
    "Nadat we deze overbodige stukken tekst hadden verwijderd, hebben we het Word-bestand via Word omgezet naar een .txt-bestand. Het omzetten naar .txt verliep echter niet soepel, en het bestand was slecht leesbaar, zoals te zien is op de afbeelding.\n",
    "\n",
    "![poging1](Afbeeldingen\\1erun.JPG)\n",
    "\n",
    "In het .txt-bestand zagen we al snel dat de tabellen niet zichtbaar waren, dus hebben we de tabellen omgezet naar tekst met een functie binnen Word. Ook dit werkte niet helemaal goed, dus hebben we ChatGPT gevraagd het .txt-bestand netjes om te zetten naar een .txt-bestand. Dit zag er al een stuk beter uit, maar het was nog steeds niet wat we wilden bereiken.\n",
    "\n",
    "![poging2](Afbeeldingen\\optie2.JPG)\n",
    "\n",
    "Na deze stap hebben we gekeken hoe Mischa's code werkt met een .txt-bestand, en op basis daarvan hebben we het .txt-bestand handmatig aangepast en getest met zijn spellingsmodel. Het .txt-bestand zag er uiteindelijk als volgt uit.\n",
    "\n",
    "![poging3](Afbeeldingen\\3erun.JPG)\n",
    "\n",
    "We merkten echter dat het model weinig uit het .txt-bestand haalde en voornamelijk op voorspellingen werkte. Dit kwam waarschijnlijk doordat de tekst niet goed per kopje op een lijn stond. Het model maakte geen gebruik van de informatie uit het .txt-bestand, dus zijn we verder gaan onderzoeken naar betere opties om tot een beter resultaat te komen.\n",
    "\n",
    "Uiteindelijk hebben we de oplossing gevonden via de volgende [generator website](https://cloud.llamaindex.ai/parse). We hebben het gefilterde .docx-bestand zonder afbeeldingen en voorblad geüpload en omgezet naar een JSON-bestand. We kozen uiteindelijk voor JSON boven .txt of .md, omdat we een [script](https://github.com/dodeeric/json-files-ingestion-into-chroma-vector-db/blob/main/app.py) vonden dat JSON-bestanden kon verwerken.\n",
    "\n",
    "Na verder onderzoek bleek ook dat JSON gemakkelijker de hoofdstukken op een logische manier indeelt, iets wat niet goed gebeurde in het .txt-bestand.\n",
    "\n",
    "Aangezien SemDB voor ons allemaal nieuw was, hebben we het proces misschien niet optimaal aangepakt, maar dit was voor ons nodig om uiteindelijk een bruikbare manier te vinden om het .docx-document om te zetten naar een JSON-bestand dat geschikt is voor onze chatbot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De volgende code is geïnspireerd door [Mischa's artikel op Medium](https://medium.com/@xvtjy/rag-implementation-using-keras-nlp-and-chromadb-34c6868dd908), we hebben eerst zijn code werkend gekregen met de spells. Daarna hebben we ons eerste concept dus draaiende gekregen met het `.txt-bestand`.\n",
    "\n",
    "Omdat niet alle code even logisch was hebben we met **ChatGPT** overal comments aan toegevoegd om beter de code te begrijpen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import de Llama library voor het werken met Llama-modellen van Llama.cpp\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# Import chromadb voor het werken met een database voor het opslaan en ophalen van vectoren en documenten\n",
    "import chromadb\n",
    "\n",
    "# Waarschuwingen negeren om een schonere uitvoer te krijgen\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import pipeline van Transformers voor het eenvoudig implementeren van verschillende NLP-taken\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lijsten om documenten, metadata en ID's op te slaan\n",
    "documents = []\n",
    "metadata = []\n",
    "ids = []\n",
    "counter = 0\n",
    "\n",
    "# Open het tekstbestand met een specifieke encoding\n",
    "with open('data/Opleidngsdeel_OER_HBO-ICT_Zwolle_2024-2025_Filterd_Modified_Long_Lines.txt', 'r', encoding='utf-8') as lines:\n",
    "    # Loop door elke regel in het bestand\n",
    "    for line in lines:\n",
    "        # Splits de regel op basis van het scheidingsteken \" - \"\n",
    "        line = line.split(\" - \")  \n",
    "        \n",
    "        # Voeg het tweede deel van de gesplitste lijn toe aan de metadata als een dictionary\n",
    "        metadata.append({'kopje': line[1]})  \n",
    "        \n",
    "        # Voeg het eerste deel van de gesplitste lijn toe aan de documentenlijst\n",
    "        documents.append(line[0])  \n",
    "        \n",
    "        # Voeg een unieke ID toe aan de IDs-lijst op basis van de teller\n",
    "        ids.append(str(counter))  \n",
    "        \n",
    "        # Verhoog de teller voor de volgende ID\n",
    "        counter += 1  \n",
    "\n",
    "# Print de lijst van ID's naar de console\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maak een client aan voor de Chroma database\n",
    "client = chromadb.Client()\n",
    "\n",
    "# Verkrijg of maak een collectie met de naam \"information\"\n",
    "collection = client.get_or_create_collection(\"information\")\n",
    "\n",
    "# Voeg de documenten, metadata en ID's toe aan de collectie\n",
    "collection.add(\n",
    "    documents=documents,    # De lijst van documenten die zijn ingeladen\n",
    "    metadatas=metadata,     # De bijbehorende metadata voor de documenten\n",
    "    ids=ids                 # De unieke ID's voor elk document\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laad een voorgetraind Llama-model\n",
    "llm = Llama.from_pretrained(\n",
    "    repo_id=\"BramVanroy/GEITje-7B-ultra-GGUF\",  # Het ID van de modelrepository\n",
    "    filename=\"geitje-7b-ultra-q8_0.gguf\",         # Het bestand van het model\n",
    ")\n",
    "\n",
    "# Maak een chat-completion aan met een voorbeeldbericht van de gebruiker\n",
    "llm.create_chat_completion(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",                    # Rol van de afzender (hier: gebruiker)\n",
    "            \"content\": \"Wat is de hoofdstad van Frankrijk?\"  # Voorbeeldvraag\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vraag de gebruiker om een vraag in te voeren\n",
    "user_input = input(\"Wat is jouw vraag?: \")\n",
    "\n",
    "# Voer een query uit op de collectie met de gebruikersvraag\n",
    "results = collection.query(\n",
    "    query_texts=[user_input],  # De tekst van de gebruikersvraag\n",
    "    n_results=1                 # Het aantal resultaten dat moet worden opgehaald\n",
    ")\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verwerk de eerste metadata en het eerste document uit de resultaten\n",
    "result = results['metadatas'][0][0]['kopje'] + \" - \" + results['documents'][0][0][:-1]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maak een prompt voor het Llama-model op basis van de resultaten en gebruikersinvoer\n",
    "prompt = f\"\"\"\n",
    "voorgestelde antwoord: {result}\n",
    "User input: {user_input}\n",
    "stel een nieuw antwoord voor op basis van het voorgestelde antwoord.\n",
    "\"\"\"\n",
    "\n",
    "# Print de prompt om te zien wat naar het model wordt gestuurd\n",
    "print(\"Prompt: \", prompt)\n",
    "\n",
    "# Genereer een nieuw antwoord met het Llama-model op basis van de gemaakte prompt\n",
    "inference = llm(prompt, max_tokens=100)  # Beperk het aantal tokens in de output\n",
    "generated_text = inference['choices'][0]['text']  # Verkrijg de gegenereerde tekst\n",
    "print(\"Inference: \", generated_text)  # Print de gegenereerde output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flask Applicatie\n",
    "\n",
    "Nadat deze code van Mischa werkte met het .txt bestand zijn we begonnen met het kijken om de Data Preparation te verbeteren. Dat is toen we erachter kwamen dat we hiervoor [LlamaParse](https://docs.llamaindex.ai/en/stable/llama_cloud/llama_parse/) konden gebruiken. Met deze website hebben we dus het `.json` bestand gegenereerd die we hebben gebruikt voor de Flask Applicatie.\n",
    "\n",
    "### Flask App Structuur voor de OERbotje Chatbot\n",
    "\n",
    "We hebben een bepaalde structuur gemaakt voor de app omdat we styling (.html/ .css) hebben toegepast. De structuur ziet er als volgt uit:\n",
    "\n",
    "```markdown\n",
    "flask_app/\n",
    "│\n",
    "├── app.py                         # Hoofd Python-bestand met de Flask-applicatie\n",
    "├── requirements.txt               # Lijst met benodigde Python-pakketten\n",
    "│\n",
    "├── templates/                     # Bevat HTML-sjablonen\n",
    "│   └── index.html                 # Hoofd HTML-bestand voor de app (Bevat HTML, CSS en JavaScript)\n",
    "│\n",
    "└── data/                          # Bevat de gegevensbestanden\n",
    "    └── OER.json                   # JSON-bestand met OER-gegevens\n",
    "```\n",
    "\n",
    "Het framework van de app hebben we gegenereerd met **ChatGPT**. We hebben deze keuze gemaakt omdat we nog nooit met 'Flask' hebben gewerkt. Na wat onderzoek ernaar hebben we dit meegegooid in een prompt samen met de casus en het notebook hierboven. met de instructie om een simpele Flask-app hiervan te maken. Dit resulteerde in een simpele Flask-app bouwen met een eenvoudige HTML-pagina waarin getypt kon worden en een knop aanwezig was om op te drukken om een antwoord te genereren.\n",
    "\n",
    "<figure>\n",
    "    <img src=\"afbeeldingen/eerste_flaskapp.png\" alt=\"Eerste Flask App\" width=\"600\"/>\n",
    "    <figcaption>Hoe de eerste versie van de flask app eruit zag.</figcaption>\n",
    "</figure>\n",
    "\n",
    "---\n",
    "\n",
    "Een groot punt nadat we de Flask-App draaiende hadden was om het antwoord te verbeteren met `Prompt Engineering;` het model hallucineerde best vaak wanneer hij niet wist waar hij in het OER moest zoeken. We hebben hem uiteindelijk daarom deze prompt meegegeven:\n",
    "\n",
    "```markdown\n",
    "*\"Je bent een chatbot die studenten helpt met vragen over het Onderwijs- en Examenreglement (OER) van Windesheim, specifiek voor de opleiding HBO-ICT.Beschrijf dit in maximaal 3 duidelijke zinnen. Zorg ervoor dat de output geen halve zinnen of ongewenste tekens aan het begin van de tekst bevat. Geef het antwoord zonder extra inleiding of kopjes en zorg ervoor dat het een volledige en samenhangende tekst is. Focus je alleen op zelfstandignaamwoorden in de vraag. Je mag niet halliciuneren. Als er geen antwoord op de vraag is geef je 'Geen resultaten gevonden'.*\"\n",
    "```\n",
    "\n",
    "Dit was iets waar we wel veel mee hebben geknutseld. Uiteindelijk waren we wel vrij tevreden met het resultaat. Uiteindelijk hebben we de interface van de app verbeterd. Hiervoor hebben we wel **ChatGPT** gebruikt omdat Flask nieuw voor ons was en omdat we allemaal weinig ervaring hebben met HTML, CSS en JavaScript.\n",
    "\n",
    "De belangrijkste dingen die we hebben toegevoegd aan de interface is dat je kan zien dat het model bezig is met een antwoord genereren en dat je de chat geschiedenis kan zien. Niet te verwarren met het feit dat het model de chat geschiedenis onthoudt bij het beantwoorden van nieuwe vragen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation \n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "Bij het evalueren van de prestaties van onze Flask-app en de onderliggende chatbot zijn er enkele belangrijke aandachtspunten naar voren gekomen.\n",
    "\n",
    "### Onbetrouwbare Informatiebronnen\n",
    "Eén van de grootste uitdagingen is dat het model soms niet de OER als bron gebruikt en in plaats daarvan zelf antwoorden verzint. Dit leidt tot antwoorden die simpelweg niet kloppen,  vooral omdat de chatbot bedoeld is om studenten nauwkeurige en relevante informatie over het Onderwijs- en Examenreglement (OER) te bieden. Ondanks onze inspanningen om `Prompt Engineering` toe te passen, blijft het model af en toe antwoorden genereren die niet op informatie uit het OER zijn gebaseerd.\n",
    "\n",
    "### Hallucinatie en Onvolledige Antwoorden\n",
    "Daarnaast hebben we gemerkt dat het model nog steeds soms hallucinaties vertoont. Soms geeft het model geen volledige zinnen terug, wat resulteert in afgekapte of onduidelijke antwoorden. \n",
    "\n",
    "### Tijdsduur voor Antwoordgeneratie\n",
    "Een ander belangrijk punt is de tijd die het model nodig heeft om antwoorden te genereren. Op dit moment duurt het ongeveer `een minuut per vraag` voordat een antwoord wordt teruggegeven. De lange wachttijd kan worden veroorzaakt door verschillende factoren, waaronder de complexiteit van de vragen, de omvang van de dataset waarmee het model werkt en de prestaties van de laptop waarop de Flask-app is gehost.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "Met de ontwikkeling van onze Flask-app zijn we ons ervan bewust dat er nog veel kansen zijn voor verdere verbetering en uitbreiding van het model:\n",
    "\n",
    "### Chatgeschiedenis\n",
    "Een eerste stap die we willen nemen, is het implementeren van een functie voor `chatgeschiedenis`. Dit zou het model in staat stellen om eerder gestelde vragen te onthouden en deze contextueel mee te nemen in de antwoorden. Hierdoor kan de chatbot relevanter en nauwkeuriger reageren op opvolgende vragen.\n",
    "\n",
    "### Meerdere Documenten Ondersteunen\n",
    "Daarnaast willen we het model uitbreiden zodat het niet alleen een OER-bot is voor de opleiding HBO-ICT, maar ook voor `andere studies`. Dit zou je kunnen doen door een structuur te ontwikkelen die het mogelijk maakt om meerdere documenten eenvoudig te integreren. Dit vereist een uitgebreidere app dan wat we momenteel hebben, maar het zou een waardevolle uitbreiding zijn die ons bereik en de functionaliteit van de chatbot aanzienlijk vergroot.\n",
    "\n",
    "### Hosting op een Externe Server\n",
    "Momenteel draait onze app nog op `localhost`, maar we overwegen om deze te hosten op een server van Windesheim, zoals Skylab of een vergelijkbare omgeving. Dit zou de toegankelijkheid van de app verbeteren en het mogelijk maken dat meer studenten gebruik kunnen maken van de chatbot.\n",
    "\n",
    "### Verbetering van het Model\n",
    "Ten slotte moet ons model in het algemeen verder `worden verbeterd`. Het is cruciaal dat de chatbot alleen antwoorden genereert op basis van de beschikbare documentatie, in dit geval de OER. Als het model geen relevant antwoord kan vinden, moet het eenvoudigweg aangeven dat het de vraag niet begrijpt en aanmoedigen om deze anders of eenvoudiger te formuleren. Deze aanpak zou niet alleen hallucinaties verminderen, maar ook de betrouwbaarheid van de chatbot verhogen, waardoor het een betrouwbaarder hulpmiddel wordt voor studenten.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "2.7.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
